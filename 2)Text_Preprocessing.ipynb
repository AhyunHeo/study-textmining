{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc32dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화(tokenization): 문서를 토큰(token)이라 불리는 단위로 나누는 작업\n",
    "# 정제(cleaning): 불필요한 단어 또는 문자를 제거\n",
    "# 정규화(normalization): 같은 의미이면서 표현이 다른 단어를 통합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908400d4",
   "metadata": {},
   "source": [
    "### 1. 토큰화(Tokenization)\n",
    "\n",
    "\n",
    "```\n",
    "토큰의 기준:\n",
    "- 보통 단어(word)를 기준, 이외에도 문자(철자) 또는 구(phase)\n",
    "- 문장(sentence), 단락(paragraph) 등을 기준으로 할 수 있음\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2412d53",
   "metadata": {},
   "source": [
    "#### 1) 단어기준 토큰나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd7aeb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\heo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'mining',\n",
       " ',',\n",
       " 'also',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'as',\n",
       " 'text',\n",
       " 'data',\n",
       " 'mining',\n",
       " ',',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'text',\n",
       " 'analytics',\n",
       " ',',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'deriving',\n",
       " 'high-quality',\n",
       " 'information',\n",
       " 'from',\n",
       " 'text',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Error: Resource punkt not found. \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text=\"Text mining, also referred to as text data mining, similar to text analytics, is the process of deriving high-quality information from text.\"\n",
    "word_tokenize(text) # 공백기준으로 나누는 듯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bad1c6",
   "metadata": {},
   "source": [
    "#### 2) 문장기준 토큰나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fd4bea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It involves the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.',\n",
       " 'Written resources may include websites, books, emails, reviews, and articles.',\n",
       " 'High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning.',\n",
       " 'According to Hotho et al.',\n",
       " '(2005) we can differ three different perspectives of text mining: information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=\"\"\"It involves the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources. \n",
    "Written resources may include websites, books, emails, reviews, and articles. \n",
    "High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. \n",
    "According to Hotho et al. (2005) we can differ three different perspectives of text mining: information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process.\"\"\"\n",
    "sent_tokenize(text) # 줄바꿈 기준으로 나누는 듯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72809f",
   "metadata": {},
   "source": [
    "#### 3) 한글 토큰나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25f356fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['텍스트',\n",
       " '분석',\n",
       " '을',\n",
       " '위해',\n",
       " '서',\n",
       " '파이썬',\n",
       " '을',\n",
       " '이용',\n",
       " '합니다',\n",
       " '.',\n",
       " '한글',\n",
       " '문장',\n",
       " '에서',\n",
       " '단어',\n",
       " '를',\n",
       " '추출',\n",
       " '하는',\n",
       " '패키지',\n",
       " '는',\n",
       " 'konlpy',\n",
       " '입니다',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt  \n",
    "\n",
    "s = \"텍스트 분석을 위해서 파이썬을 이용합니다. 한글 문장에서 단어를 추출하는 패키지는 konlpy입니다.\"\n",
    "okt=Okt()  \n",
    "okt.morphs(s) # 형태소까지 나눠줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6aff9c",
   "metadata": {},
   "source": [
    "### 2. 정제(Cleaning) / 정규화(Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669323ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 규칙기반 통합 : \"US\", \"USA\", \"United States\", …\n",
    "# 대, 소문자 통합 : \"Automobile” = “automobile”\n",
    "# 불필요 단어 제거: 출현빈도가 작은 단어 또는 길이가 짧은 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283f9c0f",
   "metadata": {},
   "source": [
    "#### 1) 텍스트 정규화\n",
    "- ```replace(a, aaa)``` : a를 aaa로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9f701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited United State from United Kingdom on 22-10-2018\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I visited US from UK on 22-10-18'\n",
    "normalized_sentence = sentence.replace(\"US\", \"United State\").replace(\"UK\", \"United Kingdom\").replace(\"-18\",\"-2018\")\n",
    "print(normalized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328caa2",
   "metadata": {},
   "source": [
    "#### 2) 불용어, 불필요 단어 제거\n",
    "- 불용어 제거 : 문장의 의미에 영향을 미치지 않는 불필요한 용어를 제거하는 작업 ex) a, am, the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60b090ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "['I', 'am', 'learning', 'Python', '.', 'It', 'is', 'one', 'of', 'the', 'most', 'popular', 'programming', 'language', '.']\n",
      "['I', 'learning', 'Python', '.', 'It', 'one', 'popular', 'programming', 'language', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\heo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = stopwords.words('english')  #영어 stopword, 한국어 지원 안됨\n",
    "print(stop_words[:10])\n",
    "\n",
    "sentence = ' I am learning Python. It is one of the most popular programming language.'\n",
    "word_tokens = word_tokenize(sentence)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "\n",
    "print(word_tokens) \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 불용어는 지원하지 않으므로 리스트 가져와 따로 작업해야 함\n",
    "# https://bab2min.tistory.com/544"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd1398",
   "metadata": {},
   "source": [
    "#### 3) 철자 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71127942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 철자 오류 예 : Procesing, insightes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9db5f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing deals with the art of extracting insights from Natural Languages.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install autocorrect\n",
    "from autocorrect import Speller # 필요 패키지 로드\n",
    "\n",
    "spell = Speller()\n",
    "sentence = \"Natral Language Procesing deals with the art of extracting insightes from Natural Languages.\"\n",
    "\n",
    "spell(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea1b5ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 내용 : 파이썬을\n",
      "대치어 : 파이선을\n",
      "도움말 : 외래어 표기법 오류(일반)입니다. 대치어를 보고 바르게 쓰십시오.\n",
      "\n",
      "\n",
      "입력 내용 : 리용합니다\n",
      "대치어 : 이용합니다|리룡합니다\n",
      "도움말 : 입력 오류입니다.\n",
      "\n",
      "\n",
      "입력 내용 : 추출하능\n",
      "대치어 : 추출하는\n",
      "도움말 : 어미의 사용이 잘못되었습니다. 문서 작성시 필요에 의해 잘못된 어미를 제시하는 상황이 아니라면 검사기의 대치어로 바꾸도록 합니다.\n",
      "\n",
      "\n",
      "입력 내용 : konlpy입니다\n",
      "대치어 : \n",
      "도움말 : 철자 검사를 해 보니 이 어절은 분석할 수 없으므로 틀린 말로 판단하였습니다.<br/><br/>후보 어절은 이 철자검사/교정기에서 띄어쓰기, 붙여 쓰기, 음절대치와 같은 교정방법에 따라 수정한 결과입니다.<br/><br/>후보 어절 중 선택하시거나 오류 어절을 수정하여 주십시오.<br/><br/>* 단, 사전에 없는 단어이거나 사용자가 올바르다고 판단한 어절에 대해서는 통과하세요!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 한국어 맞춤법 검사기1\n",
    "# https://hleecaster.com/speller-cs-pusan-ac-kr/\n",
    "\n",
    "import requests\n",
    "import json\n",
    "# 1. 텍스트 준비 & 개행문자 처리\n",
    "# with open('text.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "# text = text.replace('\\n', '\\r\\n')\n",
    "text = \"텍스트 분석을 위해서 파이썬을 리용합니다. 한글 문장에서 단어를 추출하능 패키지는 konlpy입니다.\"\n",
    "\n",
    "\n",
    "# 2. 맞춤법 검사 요청 (requests)\n",
    "response = requests.post('http://164.125.7.61/speller/results', data={'text1': text})\n",
    "\n",
    "# 3. 응답에서 필요한 내용 추출 (html 파싱)\n",
    "data = response.text.split('data = [', 1)[-1].rsplit('];', 1)[0]\n",
    "\n",
    "# 4. 파이썬 딕셔너리 형식으로 변환\n",
    "data = json.loads(data)\n",
    "\n",
    "# 5. 교정 내용 출력\n",
    "for err in data['errInfo']:\n",
    "    print(f\"입력 내용 : {err['orgStr']}\")\n",
    "    print(f\"대치어 : {err['candWord']}\")\n",
    "    print(f\"도움말 : {err['help']}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15981d",
   "metadata": {},
   "source": [
    "#### 4) 어간추출 (Stemming), 표제어(기본형) 추출 (Lemmatization)\n",
    "- 어간 추출: 단어의 의미를 담고 있는 단어의 핵심 부분을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16f9e065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "work\n",
      "work\n",
      "happier\n",
      "happiest\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "text=\"working works worked happier happiest\"\n",
    "words=word_tokenize(text)\n",
    "\n",
    "for w in words :\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c6057a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "work\n",
      "work\n",
      "happy\n",
      "happiest\n"
     ]
    }
   ],
   "source": [
    "ls = LancasterStemmer()\n",
    "text=\"working works worked happier happiest\"\n",
    "words=word_tokenize(text)\n",
    "\n",
    "for w in words :\n",
    "    print(ls.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831b4a23",
   "metadata": {},
   "source": [
    "- 표제어 추출 : 일반적으로 어간추출보다 더 정확히 어근 단어를 찾아주기 때문에 시간이 더 오래 걸리며, 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69fe0612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "work\n",
      "work\n",
      "happier\n",
      "happiest\n",
      "--------------\n",
      "working\n",
      "works\n",
      "worked\n",
      "happy\n",
      "happy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\heo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text=\"working works worked happier happiest\"\n",
    "words=word_tokenize(text)\n",
    "\n",
    "for w in words :\n",
    "    print(lemmatizer.lemmatize(w, 'v')) # v : 동사\n",
    "\n",
    "print('--------------')    \n",
    "    \n",
    "for w in words :\n",
    "    print(lemmatizer.lemmatize(w, 'a')) # a : 형용사    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f57fc",
   "metadata": {},
   "source": [
    "### 3. 한국어 전처리\n",
    "- Py-Hanspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95593ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': True,\n",
       " 'original': '안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.',\n",
       " 'checked': '안녕하세요. 저는 한국인입니다. 이 문장은 한글로 작성됐습니다.',\n",
       " 'errors': 4,\n",
       " 'words': OrderedDict([('<em', 0),\n",
       "              (\"class='green_text'>안녕하세요.</em>\", 0),\n",
       "              ('저는', 0),\n",
       "              (\"class='green_text'>한국인입니다.</em>\", 0),\n",
       "              (\"class='green_text'>이\", 0),\n",
       "              ('문장은</em>', 0),\n",
       "              ('한글로', 0),\n",
       "              (\"class='red_text'>작성됐습니다.</em>\", 0)]),\n",
       " 'time': 0.10454583168029785}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한국어 맞춤법 검사기2 \n",
    "# https://github.com/ssut/py-hanspell\n",
    "# pip install git+https://github.com/ssut/py-hanspell.git\n",
    "\n",
    "# Error : JSONDecodeError\n",
    "# https://github.com/ssut/py-hanspell/issues/31\n",
    "\n",
    "from hanspell import spell_checker\n",
    "\n",
    "sent = '안녕 하세요. 저는 한국인 입니다. 이문장은 한글로 작성됬습니다.'\n",
    "result = spell_checker.check(sent)\n",
    "result.as_dict()  # dict로 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intown1_kernel",
   "language": "python",
   "name": "intown1_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
